#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
6.867 Machine Learning Project Ideas
\end_layout

\begin_layout Section
Reinforcement Learning for Quadrotor Flips
\end_layout

\begin_layout Standard
Use reinforcement learning to teach a quadrotor how to do a flip.
\end_layout

\begin_layout Enumerate
First teach it how to do a flip.
\end_layout

\begin_layout Enumerate
Next see if it can do obstacle avoidance using output feedback with the
 quad???
\end_layout

\begin_layout Section
Q-Learning for Autonomous Driving
\end_layout

\begin_layout Standard
Basically want a car that can drive autonomously through a simple obstacle
 field.
 Alternatively could use Policy gradient (PG) methods.
\end_layout

\begin_layout Subsection
Motivation & Big Picture
\end_layout

\begin_layout Itemize

\series bold
Demo: 
\series default
Car equiped with LIDAR sensor autonomously driving through obstacle field
 without crashing.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Project would be in simulation.
\end_layout

\begin_layout Itemize
If that works would like to do it on the actual car that the UROPS are building.
\end_layout

\end_deeper
\begin_layout Itemize
Think about simple case first, then add all the bells and whistles.
\end_layout

\begin_layout Subsection
Technical Approach
\end_layout

\begin_layout Standard
Want to apply Q-Learning techniques.
 To do this need to formulate our problem as a dynamic programming problem.
\end_layout

\begin_layout Itemize

\series bold
State
\begin_inset space ~
\end_inset

Space:
\end_layout

\begin_deeper
\begin_layout Itemize
For simple case, no states needed to describe the motion of the car, i.e.
 driving at constant speed.
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $X=(x_{-n},\ldots,x_{0},\ldots,x_{n})$
\end_inset

 where 
\begin_inset Formula $x_{j}\in\mathbb{R}_{+}$
\end_inset

 is the range returned by the 
\begin_inset Formula $nth$
\end_inset

 laser beam.
 
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Action Space: 
\end_layout

\begin_deeper
\begin_layout Itemize
Simple Model: The action set is just 
\begin_inset Formula $\mathcal{A}=\{L,R,S\}$
\end_inset

 just L=Left, R=Right, S=Straight options.
\end_layout

\begin_layout Itemize
Complex Model: Allow for the car to have some state.
 Use a Dubins car model, so velocity and steering angle 
\begin_inset Formula $\theta$
\end_inset

 are the state variables corresponding to the car.
\end_layout

\end_deeper
\begin_layout Standard
Now we have specified the state and action space.
 To apply standard Q-Learning would need to discretize the state space so
 we could think about 
\begin_inset Formula $Q(X_{i},a)$
\end_inset

 for each state 
\begin_inset Formula $i$
\end_inset

.
 
\end_layout

\begin_layout Itemize

\series bold
Representations of Q-values: 
\end_layout

\begin_deeper
\begin_layout Itemize
Simple Case: Use few beams with discretized range measurements so that we
 can actually discretize the state space.
\end_layout

\begin_layout Itemize
In order to scale up we (probably) need to do something smarter than representin
g Q values with a grid.
 Suggestions offered in Gosavi survey paper.
 My intuition is that we should be able to do well enough with the linear
 representation outline below.
 Don't need to go to something as complicated as a neural network.
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Linear: 
\series default

\begin_inset Formula $Q(X,a)=\sum_{j}w_{a}(j)\phi_{j}(X)$
\end_inset

 where 
\begin_inset Formula $\phi$
\end_inset

 is a feature map.
 This is a linear representation of the Q-values.
 Makes intuitive sense to me.
 Have an idea of what the 
\begin_inset Formula $\phi$
\end_inset

 values could be.
 
\begin_inset Formula $\phi_{j}$
\end_inset

 only depends on beam 
\begin_inset Formula $x_{j}$
\end_inset

 and is a trucated version of 
\begin_inset Formula $1/x_{j}$
\end_inset

.
 Short laser detection ranges are bad, it means we are very close to an
 obstacle.
\end_layout

\begin_deeper
\begin_layout Itemize
If we allow speed to be in the state space, how does affect this representation?
??
\end_layout

\begin_layout Itemize
Would speed enter into the 
\begin_inset Formula $\phi(X)$
\end_inset

?
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Neural-Network: 
\series default
Could train a neural network to represent Q-values.
 
\end_layout

\begin_deeper
\begin_layout Itemize
I think this is going to be much harder to train.
\end_layout

\begin_layout Itemize
Could get stuck in local minima, hard to know if we have a bug or it's just
 not converging.
 Going to try to stay away from this for the moment.
 
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Subsection
Topics & Questions to Discuss
\end_layout

\begin_layout Itemize
Compare/contrast performance of the different strategies outlined above.
\end_layout

\begin_layout Itemize
Effect of having a better sensor, i.e.
 more beams.
 How does it affect .
 .
 .
 
\end_layout

\begin_deeper
\begin_layout Itemize
Training time - presumably makes it longer
\end_layout

\begin_layout Itemize
Performance - is it any better? much better?
\end_layout

\end_deeper
\begin_layout Itemize
On policy vs.
 off-policy learning.
\end_layout

\begin_layout Itemize
Even if the true model of the car is quite complex, i.e.
 Dubins, is training with simple actions 
\begin_inset Formula ${L,R,S}$
\end_inset

 and then mapping those to the full car model enough?
\end_layout

\begin_deeper
\begin_layout Itemize
As a follow up, what is the right 
\begin_inset Quotes eld
\end_inset

simple space
\begin_inset Quotes erd
\end_inset

 to train the Q-learning on, that abstracts from the full car dynamics,
 but still captures the essentials, enough so we don't crash into stuff?
\end_layout

\end_deeper
\begin_layout Itemize
How does the type/structure of environment that we train on affect performance?
\end_layout

\begin_deeper
\begin_layout Itemize
Train with convex obstacles, then what will be performance with concave
 obstacles?
\end_layout

\begin_layout Itemize
Train with concave, performance on convex???
\end_layout

\end_deeper
\begin_layout Section
Project Proposal
\end_layout

\begin_layout Subsection
Steps/Milestones
\end_layout

\begin_layout Enumerate
11/16 - Get simulation environment up and running 
\end_layout

\begin_deeper
\begin_layout Enumerate
Raycast within director
\end_layout

\begin_layout Enumerate
Simple car model (in python) -
\end_layout

\begin_deeper
\begin_layout Enumerate
At first car with constant forward speed and 3 actions, {Left, Right, Straight}
 
\end_layout

\end_deeper
\begin_layout Enumerate
Auto-generation of worlds with random collections of obstacles and boundaries.
 Should have a few canonical worlds that we can always access for testing
 repeatability
\end_layout

\begin_layout Enumerate
Simple controller
\end_layout

\begin_deeper
\begin_layout Enumerate
Maybe performs random actions for now.
 
\end_layout

\end_deeper
\begin_layout Enumerate
Have a method for logging the data of the runs? Print to a file, or broadcast
 to lcm.
 Probably print to a file is better
\end_layout

\end_deeper
\begin_layout Enumerate
11/23 Q-learning (simplified)
\end_layout

\begin_deeper
\begin_layout Enumerate
Discretize the state space, e.g.
 5 lasers with only 5 distances each, then size of state space is approximately
 10,000.
 Not that large .
 .
 .
\end_layout

\begin_layout Enumerate
Drive around in very simple world, with no obstacles, except around the
 edges to keep the car from driving off to infinity.
\end_layout

\begin_layout Enumerate
Verify performance of the code and convergence of the Q-values
\end_layout

\end_deeper
\begin_layout Enumerate
11/30 Q-Learning (full)
\end_layout

\begin_deeper
\begin_layout Enumerate
Represent the Q-values using linear regression, as outline above in section
 2.2.
\end_layout

\begin_layout Enumerate
Verify convergence of the Q-values in simple world.
\end_layout

\begin_layout Enumerate
Add obstacles and verify convergence
\end_layout

\end_deeper
\begin_layout Enumerate
12/5 Evaluation of algorithm performance
\end_layout

\begin_deeper
\begin_layout Enumerate
How does changing the number of lasers and their field of view affect
\end_layout

\begin_deeper
\begin_layout Enumerate
Training time
\end_layout

\begin_layout Enumerate
Performance
\end_layout

\end_deeper
\end_deeper
\begin_layout Subsubsection
Time permitting goals/milestones
\end_layout

\begin_layout Enumerate
Generalize our approach to a more complicated car model
\end_layout

\begin_deeper
\begin_layout Enumerate
e.g.
 allow the car to have 
\begin_inset Quotes eld
\end_inset

state
\begin_inset Quotes erd
\end_inset

 instead of the state just being the laser state
\end_layout

\end_deeper
\begin_layout Enumerate
Increase the number of lasers
\end_layout

\begin_layout Subsection
Risks
\end_layout

\begin_layout Enumerate
Q-values fail to converge, or they converge very slowly
\end_layout

\begin_deeper
\begin_layout Enumerate
Maybe because we aren't sampling the entire space
\end_layout

\begin_layout Enumerate
The state space of our Q-values is too large .
 .
 .
 
\end_layout

\begin_layout Enumerate

\series bold
Mitigation approach: 
\series default
Reduce the dimensionality of the space by applying some hand designed features
 to the laser data before it is processed by the algorithm.
\end_layout

\end_deeper
\begin_layout Subsection
Division of Responsibility
\end_layout

\begin_layout Standard

\series bold
Pete: 
\end_layout

\begin_layout Itemize
Simulator
\end_layout

\begin_layout Standard

\series bold
Lucas:
\end_layout

\begin_layout Itemize
Q-Learning (simple)
\end_layout

\begin_layout Standard

\series bold
Both:
\end_layout

\begin_layout Itemize
Q-learning (full)
\end_layout

\begin_layout Itemize
Algorithm evaluation
\end_layout

\begin_layout Itemize
Hand-tuned features for dimensionality reduction .
 .
 .
 
\end_layout

\begin_layout Section
Meeting w/ Leslie
\end_layout

\begin_layout Standard
Overall went pretty well.
 Encouraged us to start with a simple case, build it up from there.
 
\end_layout

\begin_layout Itemize
Our 
\begin_inset Quotes eld
\end_inset

state
\begin_inset Quotes erd
\end_inset

 is not really a state, i.e.
 doesn't satisfy the Markov property.
 This violates the conditions of all the convergence algorithms in RL.
\end_layout

\begin_layout Itemize
Use SARSA instead of Q-Learning.
\end_layout

\begin_deeper
\begin_layout Itemize
SARSA is apparently more tolerant to violations of the Markov property.
 The policy and action-value function move together in a sense.
 Makes things smooth .
 .
 .
\end_layout

\begin_layout Itemize
Q-Learning can be very sensitive and bounce around, probably won't converge
 very well in this scenario, but maybe worth trying if we have time.
\end_layout

\end_deeper
\begin_layout Itemize
Policy Search - another method we could try
\end_layout

\begin_deeper
\begin_layout Itemize
Have a parametric form of a policy
\end_layout

\begin_layout Itemize
Train it directly on the reward function.
\end_layout

\begin_layout Itemize
Maybe also have a linear parametric form??? Need to think about the correct
 parametric form .
 .
 .
 
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
3 Main Methods in RL:
\end_layout

\begin_deeper
\begin_layout Enumerate
Model based, i.e.
 MPC, have a model of the world and plan within it
\end_layout

\begin_layout Enumerate
Value based - e.g.
 SARSA and Q-Learning
\end_layout

\begin_layout Enumerate
Policy search
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Leslie Says:
\end_layout

\begin_deeper
\begin_layout Itemize
Implement both SARSA and policy search methods.
\end_layout

\begin_layout Itemize
Evaluate their relative performance.
\end_layout

\end_deeper
\end_body
\end_document
