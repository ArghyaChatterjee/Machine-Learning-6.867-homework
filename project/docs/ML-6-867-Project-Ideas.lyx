#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
6.867 Machine Learning Project Ideas
\end_layout

\begin_layout Section
Reinforcement Learning for Quadrotor Flips
\end_layout

\begin_layout Standard
Use reinforcement learning to teach a quadrotor how to do a flip.
\end_layout

\begin_layout Enumerate
First teach it how to do a flip.
\end_layout

\begin_layout Enumerate
Next see if it can do obstacle avoidance using output feedback with the
 quad???
\end_layout

\begin_layout Section
Q-Learning for Autonomous Driving
\end_layout

\begin_layout Standard
Basically want a car that can drive autonomously through a simple obstacle
 field.
 Alternatively could use Policy gradient (PG) methods.
\end_layout

\begin_layout Subsection
Motivation & Big Picture
\end_layout

\begin_layout Itemize

\series bold
Demo: 
\series default
Car equiped with LIDAR sensor autonomously driving through obstacle field
 without crashing.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Project would be in simulation.
\end_layout

\begin_layout Itemize
If that works would like to do it on the actual car that the UROPS are building.
\end_layout

\end_deeper
\begin_layout Itemize
Think about simple case first, then add all the bells and whistles.
\end_layout

\begin_layout Subsection
Technical Approach
\end_layout

\begin_layout Standard
Want to apply Q-Learning techniques.
 To do this need to formulate our problem as a dynamic programming problem.
\end_layout

\begin_layout Itemize

\series bold
State
\begin_inset space ~
\end_inset

Space:
\end_layout

\begin_deeper
\begin_layout Itemize
For simple case, no states needed to describe the motion of the car, i.e.
 driving at constant speed.
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $X=(x_{-n},\ldots,x_{0},\ldots,x_{n})$
\end_inset

 where 
\begin_inset Formula $x_{j}\in\mathbb{R}_{+}$
\end_inset

 is the range returned by the 
\begin_inset Formula $nth$
\end_inset

 laser beam.
 
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Action Space: 
\end_layout

\begin_deeper
\begin_layout Itemize
Simple Model: The action set is just 
\begin_inset Formula $\mathcal{A}=\{L,R,S\}$
\end_inset

 just L=Left, R=Right, S=Straight options.
\end_layout

\begin_layout Itemize
Complex Model: Allow for the car to have some state.
 Use a Dubins car model, so velocity and steering angle 
\begin_inset Formula $\theta$
\end_inset

 are the state variables corresponding to the car.
\end_layout

\end_deeper
\begin_layout Standard
Now we have specified the state and action space.
 To apply standard Q-Learning would need to discretize the state space so
 we could think about 
\begin_inset Formula $Q(X_{i},a)$
\end_inset

 for each state 
\begin_inset Formula $i$
\end_inset

.
 
\end_layout

\begin_layout Itemize

\series bold
Representations of Q-values: 
\end_layout

\begin_deeper
\begin_layout Itemize
Simple Case: Use few beams with discretized range measurements so that we
 can actually discretize the state space.
\end_layout

\begin_layout Itemize
In order to scale up we (probably) need to do something smarter than representin
g Q values with a grid.
 Suggestions offered in Gosavi survey paper.
 My intuition is that we should be able to do well enough with the linear
 representation outline below.
 Don't need to go to something as complicated as a neural network.
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Linear: 
\series default

\begin_inset Formula $Q(X,a)=\sum_{j}w_{a}(j)\phi_{j}(X)$
\end_inset

 where 
\begin_inset Formula $\phi$
\end_inset

 is a feature map.
 This is a linear representation of the Q-values.
 Makes intuitive sense to me.
 Have an idea of what the 
\begin_inset Formula $\phi$
\end_inset

 values could be.
 
\begin_inset Formula $\phi_{j}$
\end_inset

 only depends on beam 
\begin_inset Formula $x_{j}$
\end_inset

 and is a trucated version of 
\begin_inset Formula $1/x_{j}$
\end_inset

.
 Short laser detection ranges are bad, it means we are very close to an
 obstacle.
\end_layout

\begin_deeper
\begin_layout Itemize
If we allow speed to be in the state space, how does affect this representation?
??
\end_layout

\begin_layout Itemize
Would speed enter into the 
\begin_inset Formula $\phi(X)$
\end_inset

?
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Neural-Network: 
\series default
Could train a neural network to represent Q-values.
 
\end_layout

\begin_deeper
\begin_layout Itemize
I think this is going to be much harder to train.
\end_layout

\begin_layout Itemize
Could get stuck in local minima, hard to know if we have a bug or it's just
 not converging.
 Going to try to stay away from this for the moment.
 
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Subsection
Topics & Questions to Discuss
\end_layout

\begin_layout Itemize
Compare/contrast performance of the different strategies outlined above.
\end_layout

\begin_layout Itemize
Effect of having a better sensor, i.e.
 more beams.
 How does it affect .
 .
 .
 
\end_layout

\begin_deeper
\begin_layout Itemize
Training time - presumably makes it longer
\end_layout

\begin_layout Itemize
Performance - is it any better? much better?
\end_layout

\end_deeper
\begin_layout Itemize
On policy vs.
 off-policy learning.
\end_layout

\begin_layout Itemize
Even if the true model of the car is quite complex, i.e.
 Dubins, is training with simple actions 
\begin_inset Formula ${L,R,S}$
\end_inset

 and then mapping those to the full car model enough?
\end_layout

\begin_deeper
\begin_layout Itemize
As a follow up, what is the right 
\begin_inset Quotes eld
\end_inset

simple space
\begin_inset Quotes erd
\end_inset

 to train the Q-learning on, that abstracts from the full car dynamics,
 but still captures the essentials, enough so we don't crash into stuff?
\end_layout

\end_deeper
\begin_layout Itemize
How does the type/structure of environment that we train on affect performance?
\end_layout

\begin_deeper
\begin_layout Itemize
Train with convex obstacles, then what will be performance with concave
 obstacles?
\end_layout

\begin_layout Itemize
Train with concave, performance on convex???
\end_layout

\end_deeper
\end_body
\end_document
