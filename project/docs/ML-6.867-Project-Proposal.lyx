#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
6.867 Machine Learning Project Ideas
\end_layout

\begin_layout Author
Lucas Manuelli and Pete Florence
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
For our project, we want to explore reinforcement learning via Q-learning
 on a simple car model in simulation.
 We both work on robotics and so naturally are interested in scenarios in
 which we need to make decisions in order to decide which data is available
 for us to learn from, AKA reinforcement learning.
\end_layout

\begin_layout Standard
Although reinforcement learning was outside the scope of 6.867, a component
 of our proposed approach (representation of the Q-values) involves supervised
 learning.
 As a first step we will investigate linear regression (with some set of
 hand-selected features) for this component, and we may explore other supervised
 learning options as well, potentially including Bayesian priors.
 We expect this project to build strongly on what weâ€™ve learned in 6.867.
\end_layout

\begin_layout Standard
There are a few different options for baseline algorithms that we are interested
 in comparing.
 This may actually turn out to be quite easy, since there are other projects
 going on in our lab that are developing other approaches to planning in
 a simple limited-sensor-horizon car model environment.
\end_layout

\begin_layout Standard
Although our project for this class will be all in simulation, we will have
 access to a 
\begin_inset Formula $\sim$
\end_inset

 1/8 scale car hardware platform with a 2D LIDAR, and if our simulations
 work, a goal is to get the car driving around autonomously without crashing.
 
\end_layout

\begin_layout Section
Technical Approach: Q-Learning for Autonomous Driving
\end_layout

\begin_layout Standard
Want to apply Q-Learning techniques.
 To do this need to formulate our problem as a dynamic programming problem.
 
\end_layout

\begin_layout Itemize

\series bold
State
\begin_inset space ~
\end_inset

Space:
\end_layout

\begin_deeper
\begin_layout Itemize
As a simple first step suppose no states are required to describe the car.
 This would be the case if directly control the car's velocity.
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $X=(x_{-n},\ldots,x_{0},\ldots,x_{n})$
\end_inset

 where 
\begin_inset Formula $x_{j}\in\mathbb{R}_{+}$
\end_inset

 is the range returned by the 
\begin_inset Formula $nth$
\end_inset

 laser beam.
 
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Action Space: 
\end_layout

\begin_deeper
\begin_layout Itemize
Simple Model: The action set is just 
\begin_inset Formula $\mathcal{A}=\{L,R,S\}$
\end_inset

 just L=Left, R=Right, S=Straight options.
\end_layout

\begin_layout Itemize
More Complex Model: Allow for the car to have some state.
 Use a Dubins car model, so velocity and steering angle 
\begin_inset Formula $\theta$
\end_inset

 are the state variables corresponding to the car.
\end_layout

\end_deeper
\begin_layout Standard
Now we have specified the state and action space.
 Standard Q-Learning requires discretizing the state so that we can think
 about 
\begin_inset Formula $Q(X_{i},a)$
\end_inset

 for each state 
\begin_inset Formula $i$
\end_inset

.
 However, with 
\begin_inset Formula $N$
\end_inset

 laser measurements and discretizing the possible return values into 
\begin_inset Formula $K$
\end_inset

 pieces the size of our state space would be 
\begin_inset Formula $K^{N}$
\end_inset

 which grows exponentially.
 To have any hope of this working in practice we need a more efficient way
 of representing the Q-values.
\end_layout

\begin_layout Itemize

\series bold
Representations of Q-values: 
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Linear: 
\series default

\begin_inset Formula $Q(X,a)=\sum_{j}w_{a}(j)\phi_{j}(X)$
\end_inset

 where 
\begin_inset Formula $\phi$
\end_inset

 is a feature map.
 This is a linear representation of the Q-values.
 For example 
\begin_inset Formula $\phi(x)=\min(1/x,C)$
\end_inset

 for a large constant 
\begin_inset Formula $C.$
\end_inset

 Intuitively this says that we care more about laser measurements that are
 small, namely those that indicate an obstacle close by.
 This is just one example of a feature.
 We could come up with several and compare them,
\end_layout

\begin_layout Itemize

\series bold
Neural-Network: 
\series default
Could train a neural network to represent Q-values, although our intuition
 is that linear representation (with appropriately chosen featuers) may
 suffice.
\end_layout

\end_deeper
\begin_layout Section
Milestones with Proposed Timeline + Division of Responsibility
\end_layout

\begin_layout Itemize
Pete, 11/16 - Get simulation environment up and running (simple car model
 in Python, use Drake Director for visualizer, have logger working, have
 baseline hand-coded 
\begin_inset Quotes eld
\end_inset

turn away from stuff
\begin_inset Quotes erd
\end_inset

 controller)
\end_layout

\begin_layout Itemize
Lucas, 11/23 - Q-learning (simplified)
\end_layout

\begin_deeper
\begin_layout Itemize
Discretize the state space, e.g.
 5 lasers with only 5 distances each, then size of state space is approximately
 10,000 (which is not too large).
\end_layout

\begin_layout Itemize
Drive around in very simple world, with no obstacles, except around the
 edges to keep the car from driving off to infinity.
\end_layout

\begin_layout Itemize
Verify performance of the code and convergence of the Q-values.
\end_layout

\end_deeper
\begin_layout Itemize
Lucas + Pete, 11/30 - Q-Learning (full)
\end_layout

\begin_deeper
\begin_layout Itemize
Represent the Q-values using linear regression, as outline above.
\end_layout

\begin_layout Itemize
Verify convergence of the Q-values in simple world.
\end_layout

\begin_layout Itemize
Add obstacles and verify convergence.
\end_layout

\end_deeper
\begin_layout Itemize
Lucas + Pete, 12/5 - Evaluation of algorithm performance (change number
 of lasers, their FOV (field of view), see how training time and performance
 (measured as time without crashing?) are affected.
\end_layout

\begin_layout Section
Risks
\end_layout

\begin_layout Standard
Q-values fail to converge, or they converge very slowly.
 Possible reasons for failure to converge could be that we aren't sampling/cover
ing the entire state space in our simulations.
 One approach to reducing the dimensionality of the state space is to reduce
 the number of lasers, e.g.
 from 50 to 25.
 A second option is to summarize the entire laser measurement 
\begin_inset Formula $X=(x_{-n},\ldots,x_{0},\ldots,x_{n})$
\end_inset

 into a small set of hand designed features.
 For example, these features could include the range and bearing to the
 nearest obstacle.
 Then we could try to express the Q-values as a function only of this small
 set of features.
 This would mean we are estimating/learning a much smaller set of parameters.
\end_layout

\begin_layout Section
Topics/Questions to Discuss In Report
\end_layout

\begin_layout Itemize
Compare/contrast performance of the different strategies considered.
\end_layout

\begin_layout Itemize
What is the effect of having a better sensor, i.e.
 more beams.
 How would this affect training time (presumably makes it longer) and performanc
e? Is there a way to train with few beams (to get fast learning), but still
 be able ot use the added information of many beams if they are available
 in practice?
\end_layout

\begin_layout Itemize
How does on policy vs.
 off-policy learning compare in this setup, e.g.
 Q-Learning vs.
 SARSA.
\end_layout

\begin_layout Itemize
Even if the true model of the car is quite complex, i.e.
 Dubins, can we train with simple actions 
\begin_inset Formula ${L,R,S}$
\end_inset

 and then use the policy learned on the simple model to come up with a policy
 on the full model? As a follow up, is there an appropriate 
\begin_inset Quotes eld
\end_inset

simple space
\begin_inset Quotes erd
\end_inset

 on which to train the Q-learning, that abstracts from the full car dynamics,
 but still captures the essentials, enough so we don't crash into stuff?
\end_layout

\begin_layout Itemize
How does the type/structure of environment that we train on affect performance?
 If we train our algorithm in a world with convex obstacles, what will be
 the performance in a world with concave obstacles and vice versa?
\end_layout

\end_body
\end_document
