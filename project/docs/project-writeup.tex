%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2015 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2015,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{todonotes}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2015} with
% \usepackage[nohyperref]{icml2015} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\def\code#1{\texttt{#1}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
% \usepackage{icml2015} 

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
\usepackage[accepted]{icml2015}

\usepackage{enumitem}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amsfonts}


\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother




% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:

% Saving space by deleting running title (does this actually save space?)
%\icmltitlerunning{Submission and Formatting Instructions for ICML 2015}

\begin{document} 

\twocolumn[

% Saving space by deleting title
%\icmltitle{Submission and Formatting Instructions for \\ 
%           International Conference on Machine Learning (ICML 2015)}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2015
% package.
\icmlauthor{Lucas Manuelli}{manuelli@mit.edu}
\icmladdress{Massachusetts Institute of Technology}
\icmlauthor{Pete Florence}{peteflo@mit.edu}
\icmladdress{Massachusetts Institute of Technology}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{6.867, machine learning}

\vskip 0.3in
]

% Saving space by deleting abstract
%\begin{abstract} 
%The purpose of this document is to provide both the basic paper template and
%submission guidelines.
%\end{abstract} 

\section{Introduction}
In this project we evaluate the ability of several reinforcement learning methods to autonomously drive a car through a cluttered environment. We don't assume a map of the environment a priori but rather rely on our sensor (in this case a LIDAR) to make control decisions. A standard approach to solving this problem would require building up a map of the environment, possibly using an occupancy grid, and then planning a collision free path through this estimated map using a planner such as RRT*. On the other hand one notes that simple output feedback controllers such as Braitenberg controllers, can be quite effective for obstacle avoidance tasks. Guided by this example we want to see how reinforcement learning performs in learning an output feedback controller.

\section{Experiment Setup and Simulation Environment}

\subsection{Car Model}

For our car model we consider a simplified Dubin's car model. The location of the vehicle is described by $(x,y,\theta)$, where $(x,y)$ denote Cartesian position and $\theta$ denotes the orientation. For our purposes we suppose that we can control the derivative of the orientation, namely our control is $u = \dot{\theta}$. We also suppose a fixed speed $v$. Then the dynamics of the vehicle are given by
\todo{figure illustrating car + lidars}
%
%
\begin{align}
\dot{x} &= v \cos(\theta) \\ 
\dot{y} &= v \sin(\theta)\\ 
\dot{\theta} &= u
\end{align}
%
%
%

\subsection{Sensor Model}
For our sensor model we use a small array of point LIDARs. In particular, $N$ beams are spread evenly over the field of view $[\theta_{min},\theta_{max}]$.  All experiments shown use $N=20$ and $[\theta_{min}=-\pi,\theta_{max}=\pi]$    Each laser has a maximum range of $d_{max}$. The $n^{th}$ laser then returns $x_n$, which the distance to the first obstacle it encounters, or $d_{max}$ if no obstacle is detected. Then one laser measurement can be summarized as $X = (x_1,\ldots,x_N)$.

\subsection{Obstacle Environment}
Picking the right obstacle environment for our task required some trial-and-error.  Obstacles that were too thin along one direction were not a good choice, as they could hide in between the ``array of point LIDARs'' as the robot approached.  Thus circular obstacles were chosen, as they circumvented this problem.  Picking the right density of obstacles and obstacle size was another trial-and-error process.  A listing of approximate parameters for the obstacle environments used is provided below.  Generalizations across obstacle environments it outside the scope of this project.

\scalebox{0.7}{
  \begin{tabular}{l|c|c|l|c|c|c|c|c}
	\textbf{Car parameters}\\
	Car velocity, $v$ & 16 m/s \\ 
	Maximum turning rate, $u_{max}$ & 4 rad/s\\
	\\
	
	\textbf{Sensor parameters}\\
	Number of point LIDARS, N & 20 \\
	Maximum laser range, $d_{max}$ & 10 m \\
	Field of view $[\theta_{min},\theta_{max}]$ & $[-\pi, \pi]$ \\
	\\
	
	\textbf{Obstacle environment parameters}\\
	Area of square world & 250 $m^2$ \\
	Obstacle density & 0.18 / $m^2$ (45 obstacles in 250 $m^2$)\\
	Circular obstacle radius & 1.75 m \\
	
  \end{tabular}
}

\subsection{Simulation Environment}

Our software stack is integrated with a robotics visualization tool called Director \todo{director citation}. We use the Director code for visualization and raycasting. This is the only external code that we use in addition to standard Python modules. Our simulation environment is written in Python. We have a main class called \code{Simulator} which combines together several other modules, e.g. \code{Car}, \code{Sensor}, \code{World}, \code{Controller}, \code{Reward}, \code{Sarsa}, etc, to construct a working simulation class. All of this code was written by us for the project. As mentioned before, the only fundamental pieces of code that we didn't write were the visualization tools and the raycasting method which we use for computing our sensor model. One nice feature of the Director application is that it uses Visualization Toolkit as the underlying graphics engine, and thus our raycast calls are actually in C++ and thus very efficient. We use a timestep of $dt = 0.05$ in all our experiments, and \code{spicy.integrate} is used to for integrating forward the dynamics.

\todo{add world generation details}

\subsection{Braitenberg (Hand-Designed) Controller}
\label{default_controller}
As a baseline against which to compare our learned controllers, we used a simple hand-designed controller inspired by the controllers of Valentino Braitenberg.  The simplest controller we tried simply counts the number of non-max sensor returns on the left ($n_{left} = \sum_i^{N/2} \mathbbm{1}[x_i < d_{max}]$) and compared to the number of non-max sensor returns on the right ($n_{right} = \sum_{i=N/2+1}^N \mathbbm{1}[x_i < d_{max}]$), and then selected the action to turn towards the direction $min(n_{left}, n_{right})$.  If no obstacles were seen ($n_{left} = n_{right} = 0$), then the controller would select to go straight.  Thus the action space for this simple controller is $A = \{-u_{max}, 0, u_{max} \}$.  This controller actually works surprisingly well.

The best Braitenberg-style controller we used is a slight improvement on the above version, but actually takes into the account the distances measured.  This controller uses the squared inverse of each of the sensor measurements as its features: $\phi_{B} (x_i) = \frac{1}{x_i^2}$.  The sum of these features is then computed for the left and right, and again the action is selected to turn towards the direction $min(n_{left}, n_{right})$.  An algorithm description is provided:
%
\begin{algorithm}[H]
   \caption{Braitenberg Squared Inverse Controller (BSIC)}
   \label{alg:bsic}
\begin{algorithmic}
   \STATE {\bfseries Input:} sensor data $x_i$, size $N$
   \STATE {\bfseries Output:} u from $\mathcal{A} =  \{-u_{max}, 0, u_{max}\}$
   \STATE Initialize $n_{left}$, $n_{right} = 0$
   \FOR{$i=1$  {\bfseries to} $N/2$}
   \STATE $n_{left} = n_{left} + 1/(x_i)^2$
   \ENDFOR
   \FOR{$i=N/2 + 1$  {\bfseries to} $N$}
   \STATE $n_{right} = n_{right} + 1/(x_i)^2$
   \ENDFOR
   \IF {$n_{left} > n_{right}$}
   \STATE $u = -u_{max}$ (TURN LEFT)
   \ELSE 
   \IF {$n_{left} < n_{right}$}
   \STATE $u = u_{max}$ (TURN RIGHT)
   \ELSE 
   \STATE $u=0$ (GO STRAIGHT)
   \ENDIF
   \ENDIF

\end{algorithmic}
\end{algorithm}



\section{Dynamic Programming: SARSA and Q-Learning}
Two of the most popular methods in reinforcement learning are SARSA and Q-Learning. These methods both aim to find the optimal policy of a dynamic programming problem. Thus the first step is to reformulate our problem as a dynamic programming problem. 

\subsection{Dynamic Programming Formulation}
The first step to formulating our as a dynamic programming problem is to define a ``state.'' Let $S_c = (x,y,\theta)$ be the state of the car, and let $X  = (x_1,\ldots,x_n)$ be the sensor state. In order to satisfy the Markov property, our reinforcement learning agent would also need to have access to the full state of the world, $S_{world}$, i.e. a complete description of each of the obstacles.  To make our problem similar to the limitations of a real car robot with just the described LIDAR sensor model, we only allow our reinforcement learning agent to access $X$, and so we consider $S = X$. Since this is not a state in the true sense of the word let us call it a reward state.  Next we need to define our action set. In principle our model has continuous action set. However, for the purposes of SARSA and Q-Learning it is much easier to have a discrete action space. Thus we restrict ourselves to $a \in \{-4,0,4\} = \mathcal{A}$, which correspond to left, straight, and right actions. Let $\theta_n$ be the angle of $n^th$ laser where $\theta_n = 0$ corresponds to straight ahead. Now we need to define the reward function. First define weights
%
%
\begin{equation}
w_n = r_0 + r_1 \cos(\theta_n)
\end{equation}
%
%
Also define
%
%
\begin{equation}
\phi(x_n) = \begin{cases}
\min(1/x_n, C_{max}) & x_n < d_{max}\\
0 & x_n = d_{max} 
\end{cases}
\end{equation}
%
%
Then $\phi$ has the effect of amplifying short distance measurements. Let $W = (w_1,\ldots,w_N), \phi(X) = (\phi(x_1),\ldots,\phi(x_n))$ Then define the reward $R(S,a)$ as
%
%
\begin{equation}
R(S,a) = C_{action} |a| + \langle W, \phi(X) \rangle
\end{equation}
%
%
where $C_{action}$ is an action cost. Finally if the car is currently in collision with an obstacle, i.e. some laser $x_n$ is reading less than $d_{min}$ then we set $R(S,a) = C_{collision}$.

Now let us think about why this may be a reasonable reward function for our objective, which is obstacle avoidance. One should think of $r_0,r_1, C_{action}, C_{collision} < 0$ and $C_{max} > 0$. Then we see that the components of $\phi(X)$ increase as we get close to obstacles. And they increase as the inverse of the distance to the obstacle. Thus this is penalizing getting too close to obstacles. Now consider the weights $W$. They have the following form \todo{include picture}. This means that it's worse to have obstacles in front of you than off to the side. The action cost is to encourage the car to drive straight rather than spin around in circles. Finally since we are interested in obstacle avoidance we impose a large penalty if the car does crash into an obstacle.

The reason we have $\phi(X)$ and the weights $W$ is reward shaping. In reality all we care about is not crashing into obstacles, but it helps the algorithms to converge if the rewards ``guide'' them towards staying away from obstacles, e.g. if the reward is higher the further you are from the obstacles. 

Now that we have the reward function we can formulate our problem as a dynamic program. The problem is to find the policy $\pi:S \to \mathcal{A}$ to solve
%
%
\begin{equation}
\max_{\pi} E_{\pi} \left[ \sum_{t \geq 0} \gamma^t R(S_{t+1}, a_t) \right]
\end{equation}
%
%
The expectation is over the reward states that result from taking actions according to policy $\pi$. Here $\gamma \in (0,1)$ is a discount factor. In a standard dynamic programing framework the state $S_t$ would be markov. That is, future states depend only on the current state and future actions. In our case however our reward state is non-markov since the map of the world is not included as part of our state. 

\subsection{SARSA($\lambda$)}
\label{discrete_sarsa}
The difficulty with applying standard dynamic programming techniques to the current problem is that it is hard to write down the transition law $(S_t,a_t) \to S_{t+1}$ since this requires knowing how the sensor measurements will evolve. The SARSA technique allows us to circumvent this problem if we have access to runs/simulations of the true system. The Q-Values are defined as 
%
%
\begin{equation}
q_{\pi}(S,a) = E_{\pi} \left[ \sum_{k \geq 0} \gamma^k R(S_{t+k+1}, a_{t+k}) | S_t = s, A_t = a \right]
\end{equation}
%
%
Then if we knew the Q-values we could choose the best policy as 
%
\begin{equation}
\label{q-policy}
a = \arg \max_{a'} q_{\pi}(s,a) 
\end{equation}
%
The ultimate goal of SARSA($\lambda$). Is to find the optimal policy $\pi^*$. The approach is to estimate $q_{\pi^*}$. Then the optimal policy is found by appling equation (\ref{q-policy}). Using the Bellman equation for the Q-Values the SARSA($\lambda$) update is given by \todo{use algorithmic package to do description of SARSA() here }


In order to apply SARSA in the form described above we need to discretize the state-action space. In section \ref{sarsa_function_approximation} we describe an approach that doesn't require discretizing the state. In order for the SARSA algorithm to be effective without drastically increasing the training time, we must keep the size of the discretization relatively small. With this in mind we chose a discretization based on bins. The discretization is paramaterized by three parameters. $N_{inner}$, the number of inner bins, $N_{outer}$ the number of outer bins and $\beta_{cutoff}$, the cutoff distance between inner and outer bins. \todo{could use a picture here that describes the discretization}.


Bins are labeled with a $1$ if they are occupied. We have already discretized the action space into $\mathcal{A} = \{-4,0,4\}$. Thus the discretization of the state-action space lies in the space $\{0,1\}^{N_{inner} + N_{outer}} \times \mathcal{A}$. Thus it has size $2^{N_{inner} + N_{outer}} \times |\mathcal{A}|$.



\subsubsection{Results}

There are many parameters that must be chosen in order to run the SARSA lambda algorithm. We discuss some of the most important here and give intuition on how we chose them. Then we will analyse how varying a few key parameters (such as $\lambda$) around our baseline setup affects performance. show plots illustrating  Then we show plots of performance as we vary certain key parameters.

Two of the most important parameters are the number of bins $N_{inner}, N_{outer}$ in the discretization. You need enough bins that so that the state is informative enough to pass through relatively narrow gaps between obstacles. However adding more bins comes at the cost of increasing the size of the state space which can increase training time. In addition if a particular action-state pair is not visited sufficiently many times then that Q-value $Q(s,a)$ will not be a good approximation to the true Q-value, and hence control decisions taken based on that Q-value will not be optimal (and in fact will be quite poor usually). Through many experiments we found that $N_{inner} = 5, N_{outer} = 4$ provided good performance while keeping the size of the discretized state-action space relatively small at $2^9 * 3 = 1536$. Another set of parameters that had a large impact on performance were those of the cost function. It turned out to be important to balance the reward for staying away from obstacles with the penalty for using large control action, $C_{action}$. If $C_{action}$ was too small the controller could end up learning to spin in circles, see Section \ref{failure_modes} for more discussion of this problem. Alternatively if $C_{action}$ was too high the behavior would bias too much towards driving straight and wouldn't do enough to avoid obstacles. Another important parameter was the step size $\alpha$ in the SARSA update. It needs to be small enough so that the Q-Value estimates don't diverge. However, if it is too small then the Q-values are updated only a small amount in each iteration and convergence would take a long time. For the discrete state-action case we found that $\alpha = 0.2$ worked well, although it is difficult to evaluate what the ``correct'' step size is. See Table \todo{make a table with default parameter values, put it in the appendix} for our preferred set of parameters.

First we provide a qualitative discussion of the results of the Q-Learning using parameters from Table \todo{make a table with default parameter values, put it in the appendix}. We initialize all of the Q-value to zero and run the SARSA algorithm for 8000 seconds. Since our simulation timestep is $dt = 0.05$ this amounts to $8000/dt = 160,000$ iterations of the SARSA update. Our simulation runs at approximately $700$ Hz. Since it takes $20$ ticks of the simulator to complete one second this amounts to a simulation of 35 $\times$ normal speed. Thus this training takes approximately $3.8$ minutes. The controller that we get out of this performs reasonably well. Specifically it manages to drive around the obstacle field shown in figure \todo{picture of the obstacle field} at a reasonably high speed with mean time between crashes of \todo{figure this out, currently it is 600} 30 seconds. One thing to note is that this is much worse than the performance of our default controller described in section \ref{default_controller}. At the given car speed in this run the default controller can essentially drive around indefinitely without crashing. However, it's behavior is to make a hard turn as soon as it detects an obstacle. Thus it has a tendency to get trapped within an area just going in circles as in figure \todo{show figure}. Since the default controller turns very agressively it never will go through a small gap. On the other hand since our reward function penalizes taking control actions (by $C_{action}$) it rewards our controller for going straight. Thus the learned controller will continue to go straight in some situations where we are detecting obstacles, but they are not in our direct path see figure \todo{include graphic}. This is an interesting behavior that is learned. Thus qualitatively our learned controller turns much less than the default controller and thus doesn't tend to get stuck circling in one spot. Another interesting feature of the controller is that learns how to drive through narrow gaps, see figure \todo{include graphic}.

For a more quantitative analysis we can consider plots showing the average and discounted reward. The plots show that as time progresses the algorithm improves it's performance as is to be expected. Both the discounted reward and the run duration increase over time as the learning improves. 


\subsubsection{Failure Modes}
\label{failure_modes}

Occasionally our learned controller would end up spinning us in circles, quite literally. Now we try to understand why this might have happened. What actually happened is that for the case where there are no controller would end up spinning us in circles. Let $X_{max} = (d_{max}, \ldots, d_{max})$ which corresponds to a laser measurement where no obstacles are detected. Then what is happening is that $Q(X,0) < \max_{a \in \{-4,4\}} Q(X,a)$. Thus we end up turning a given direction, suppose it is left without loss of generality, even when we don't see an obstacle. The reason this actually works quite well is that in our randomly generated maps there tends to be pockets of space that are obstacle free. If the vehicle happens to enter or be initialized into one of these ``obstacle free'' zones then by aggressively turning a given direction it can keep itself inside the ``obstacle free'' zone and avoid ever running into something. Although this controller actually produces a high reward (since it never crashes) it is not what one would consider a ``good'' controller. There are several reasons that SARSA may end up learning this behavior. The most important is that our reward-state is not Markov. This means that when we are in reward state $X_{max}$ we don't know our transition probabilities to the next reward state. This is because we have only local information in the reward-state and have no information about the global map. This is an example of our learning algorithm exploiting structure in the problem that we didn't mean for it to take advantage of.

Possible solutions to this problem are to redesign the reward function. We could include reaching a goal as part of the reward function, so then the learning algorithm would have an incentive to make progress towards this goal rather than spin in circles. Another solution approach is to increase the number of obstacles in the world to eliminate these ``obstacle free zones.''


\subsubsection{Analysis}

In this section we perform two experiments to further characterize the performance of our SARSA algorithm. In the first experiment we hold all the parameters fixed except $\lambda$. The $\lambda$ parameter affects the eligibility traces and controls the balance between a TD (temporal difference) update and an MC (monte carlo update). At the two extremes, $\lambda = 0$ corresponds to a pure TD update and $\lambda = 1$ corresponds to a pure MC update. Intuitively TD updates rely heavily on the markov property and the Bellman equation for the Q-Values. On the other hand MC updates simply try to estimate $Q(S,a)$ using the empirically observed values and don't rely on the markov property or the Bellman equation. It is known that \todo{include reference} that $\lambda > 0$ can help convergence speed and may improve performance in non-markov domains. In Figure \todo{include figure ref} we see a plot of performance data several different $\lambda$ values. Since each run takes $5$ minutes to train we are limited in the number of simulations we can perform. Overall the trend seems to be that higher lambda values provide better performane. Since our observation state is highly non-markov this coincides with the fact that SARSA($\lambda$) with $\lambda$ close to $1$ is less sensitive to violations of the Markov property since each step is more similar to a MC update.


Another interesting fact that we noticed is that, even with the same set of parameters, different runs of the learning algorithm can produce quite varied performance. Figure \todo{include reference} shows several runs of the learning algorithm using the same set of parameters. Out of the 10 trials, runs 5 and 9 standout. They have extremely long average run drations which are an order of magnitude larger than all the rest. Investigating these runs more carefully they both succumbed to the ``drive in circles'' failure mode. Thus our learning algorithm is sensitive two this failure mode, with about a $20\%$ failure rate. We believe that with a more carefully designed reward function we could take care of this failure mode.


\subsection{Watkins Q-Learning}
\label{discrete_q_learning}

Q-Learning is almost the same as SARSA but with a slight variation in the update. While SARSA is an on-policy learning method (that is we are learning the Q-value for the current policy), Q-Learning is an off-policy method (we are learning the Q-Values under the optimal policy (even though we not currently controlling according that policy). The same preferred parameter set for SARSA given in Table \todo{reference} also works well for Q-Learning. The resulting controller exhibits similar qualitative performance to the SARSA controller described above. However the quantitative performance seems to be better.





\subsection{SARSA with function approximation}
\label{sarsa_function_approximation}

One of the major drawbacks of the methods presented in sections \ref{discrete_sarsa} and \ref{discrete_q_learning} is that they require discretizing the state-action space. In reality however our state is really continuous since the laser measurements really lie in $\mathbb{R}_+^N$. An alternative to discretizing the state space is to use function approximation to approximate the Q-Values. As described in Chapter 9 of \todo{Barto Sutton ref} this still requires a discrete action space. We consider a linear function approximation to the Q-Values given by
%
%
%
\begin{equation}
\label{q_value_approx}
Q(S,a) = w_a(0) + \sum_{n = 1}^N w_a(n)\phi(x_n)
\end{equation}
%
%
We think this is a reasonable approximation to the q-values since it has almost the same form as the reward function. We include an intercept term $w_a(0)$ to capture the fact that even when the feature vector $\phi(X)$ is identically zero we prefer action $a = 0$ to actions $a=4,-4$. In this case the SARSA($\lambda$) algorithm is given by
%
%
\todo{describe algorithm here using algorithmic package}
%
%
Intuitively this corresponds to adjusting the weights $w_a(n)$ to get the best approximation to the Q-Values in a least squares sense (see Button-Sarto Ch. 9 for a precise statement) \todo{Barto-Sutton ref}. 


\subsubsection{Results}

It is notoriously difficult to get function approximation methods for SARSA to perform well. Performance fundamentally depends on whether the chosen parametric form, given by (\ref{q_value_approx}), provides a good approximation of the Q-Values. Ultimately we were not able to get a meaningful controller out of this approach. In what follows I will try to describe why this might be and what the lessons learned were. the fist thing we noticed was that a much smaller step-size $\alpha$ was needed. After experimenting we found that $\alpha = 1e-4$ was small enough that the weights wouldn't diverge to $\infty$. For the training in sections \ref{discrete_sarsa} and \ref{discrete_q_learning} we initialized the Q-Values to zero. This approach didn't yield very good results for the function approximation SARSA method \todo{maybe need to elaborate on this, we end up spinning in circles etc.} An alternative, as outline in \todo{reference Leslie's practical reinforcement learning for mobile robots paper} to help the learning algorithms perform better is to run a known controller to help learn some baseline Q-Values. Essentially during a supervised learning period we are learning weights $\{w_a(n)\}$ which approximate the Q-Values under the known policy $\pi_{default}$. During this phase we are performing the weight updates from \todo{reference cts sarsa figure} but the policy is chosen according to $\pi_{default}$ rather than the epsilon greedy policy. In this setup we run this supervised training for a while before switching the standard SARSA($\lambda$) updates specified in \todo{table}. If we use the standard environment used in sections \ref{discrete_sarsa} and \ref{discrete_q_learning} we end up falling into the ``drive in circles'' failure mode outline in \ref{failure_modes}. Increasing the obstacle density to eliminate the ``obstacle free'' zones resolves this problem and we are able to get a functioning controller. The controller performs reasonably well, but still worse than a QLearning controller on the same environment \todo{need to verify this}. The interesting thing is to attempt to understand the weights $\{w_a(n)\}$ that we are learning and why they might be working. The weights are plotted in Figure \ref{figures/sarsa_cts_weights.png}. The actions ${4,0,-4}$ correspond to Left, Straight, and Right, respectively. Also the laser measurements go from $-10$ to $10$ with $0$ corresponding to straight ahead. Thus $x_{-10}$ is our leftmost laser measurement. Not shown are the intercept terms $w_a(0)$. The greedy controller that can be derived from the Q-Values is $\pi(S) = \arg \max_{a'} Q(S,a')$. In other words we should choose the action that maximizes the Q-Value for the current state. Suppose there is an obstacle to our right, then the feature vector $\phi(X)$ will have $\phi(x_n)$ small for $n < 0$ (i.e. range measurements to our left are large) and $\phi(x_n)$ large for measurements to our right where the obstacle is. Since the weights corresponding to action $a = 4$, which is Left, are mostly positive for $w_4(n) > 0$ for $n > 0$, and since the only components of $\phi(X)$ which are large are those where the obstacle is (i.e. $n > 0$) we get that $w_{4}(n) \dot \phi(X)$ is moderately positive. On the other hand $w_{-4}(n)$ is exactly the opposite. For $n > 0$ we mostly have $w_{-4}(n) < 0$ and so $w_{-4}(n) \dot \phi(X)$ will most likely be negative. Thus we have that $Q(S,4) > Q(S, -4)$. So we prefer to go left rather than right. A similar logic applies to understanding why $Q(S,-4)$ could be larger than $Q(S,0)$. Thus in this case we would choose left. 

The above exposition gets the general idea across. However, as can be seen in Figure \ref{figures/sarsa_cts_weights.png} the actual weights, while exhibiting some general trends, are quite messy and hard to parse.

%
%
% \begin{align}
% &\delta \leftarrow R + \gamma Q(S',A') - Q(S,A)\\
% & Z(S,A) \leftarrow Z(S,A) + 1\\
% \text{for all } s \in \mathcal{S}, a \in \mathcal A
% \end{align}
% %
% %



\subsection{Failure Modes}
\label{failure_modes}
We end up spinning around in circles. Maybe this is because we are assuming we are Markov when really we are not?


 so that we penalize being close to obstacles, i.e. 

\section{Conclusion}

\section{Division of Labor}

\section{Figures}

\begin{figure}
\centering
\includegraphics[scale=0.5]{figures/sarsaDiscrete_lam_0_7_6500_bar_all_controllers.png}
\caption{$\lambda = 0.7$}
\label{figures/sarsaDiscrete_lam_0_7_6500_bar_all_controllers.png}
\end{figure}

\begin{figure}
\centering
\includegraphics[scale=0.5]{figures/sarsaDiscrete_lam_0_7_6500_time_series_learned_controllers.png}
\caption{$\lambda = 0.7$}
\label{figures/sarsaDiscrete_lam_0_7_6500_time_series_learned_controllers.png}
\end{figure}


\begin{figure}
\centering
\includegraphics[scale=0.5]{figures/sarsa_cts_weights.png}
\caption{$\lambda = 0.7$}
\label{figures/sarsa_cts_weights.png}
\end{figure}



\bibliography{example_paper}
\bibliographystyle{icml2015}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
