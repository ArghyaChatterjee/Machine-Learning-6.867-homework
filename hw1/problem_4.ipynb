{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt\n",
    "import os\n",
    "from linear_regression import LinearRegression\n",
    "from gradient_descent import GradientDescent, quad, quadGrad\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SAEwReg() takes at least 3 arguments (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0ca3ebcadcb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradDescentSAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-0ca3ebcadcb9>\u001b[0m in \u001b[0;36mgradDescentSAE\u001b[0;34m(M)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstepSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputeMin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprintSummary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstoreIterValues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museGradientCriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxFunctionCalls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotIterValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pflomacpro/Desktop/Classes/6.867/Machine-Learning-6.867-homework/hw1/gradient_descent.pyc\u001b[0m in \u001b[0;36mcomputeMin\u001b[0;34m(self, x_initial, maxFunctionCalls, useGradientCriterion, storeIterValues, printSummary)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mx_current\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_initial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mf_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevalF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_current\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pflomacpro/Desktop/Classes/6.867/Machine-Learning-6.867-homework/hw1/gradient_descent.pyc\u001b[0m in \u001b[0;36mevalF\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevalF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumFunctionCalls\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevalGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: SAEwReg() takes at least 3 arguments (2 given)"
     ]
    }
   ],
   "source": [
    "def plot(lr,w, plot_sin=True, plot_test=True, plot_validate=True):\n",
    "    # plot sin(2*phi*x) in green\n",
    "    x_min = np.amin(lr.x)\n",
    "    x_max = np.amax(lr.x)\n",
    "    x = np.linspace(-3,3,1000)\n",
    "    M = lr.numFeatures - 1\n",
    "    \n",
    "    lr_temp = LinearRegression(x,x,lr.numFeatures-1)\n",
    "    reg_prediction = np.dot(lr_temp.phi,w)\n",
    "    \n",
    "    lr_validate = LinearRegression.fromFile(validate_filename, M)\n",
    "    lr_test = LinearRegression.fromFile(test_filename, M)\n",
    "    \n",
    "     \n",
    "    plt.plot(x, reg_prediction, color='r')\n",
    "    \n",
    "    # the training set is plotted in blue\n",
    "    plt.scatter(lr.x, lr.y, color='r', marker='o',facecolors='none')\n",
    "    \n",
    "    # test set plotted in green\n",
    "    if plot_test:\n",
    "        plt.scatter(lr_test.x, lr_test.y, color='g', marker='o',facecolors='none')\n",
    "        \n",
    "    # validation set plotted in orange\n",
    "    if plot_test:\n",
    "        plt.scatter(lr_validate.x, lr_validate.y, color='b', marker='o',facecolors='none')\n",
    "              \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def gradDescentSAE(M):\n",
    "    lr = LinearRegression.fromFile(filename, M)\n",
    "    w_initial = -1*lr.reg()\n",
    "    w_initial = 0.0*lr.reg()\n",
    "    gd = GradientDescent(lr.SAEwReg)\n",
    "    gd.stepSize = 0.05\n",
    "    gd.tol = 1e-8\n",
    "    (w, sae, _, _) = gd.computeMin(w_initial, printSummary=True, storeIterValues=True, useGradientCriterion=False, maxFunctionCalls=50000)\n",
    "    gd.plotIterValues()\n",
    "    \n",
    "    res = opt.minimize(lr.SAEwReg, w_initial)\n",
    "    print \" \"\n",
    "    print \"--- Scipy Minimization Summary --- \"\n",
    "    print \"x_min is = \" + str(res.x)\n",
    "    print \"f_min is = \" + str(res.fun)\n",
    "    print \"numFunctionCalls = \" + str(res.nfev)\n",
    "#     print \"numIterations = \" + str(res.nit)\n",
    "    print \"---------------------------- \"\n",
    "    print \" \"\n",
    "    \n",
    "    print \"|w_gd - w_scipy|^2 is\"\n",
    "    print np.linalg.norm(w - res.x)\n",
    "    \n",
    "    w_reg = lr.reg()\n",
    "    print \" \"\n",
    "    print \"--- Linear Regression Results ---\"\n",
    "    print \"x_min is = \" + str(w_reg)\n",
    "    print \"f_min is = \" + str(lr.SAEwReg(lr.reg()))\n",
    "    \n",
    "    print \"|w_gd - w_ols|^2 is\"\n",
    "    print np.linalg.norm(w - w_reg)\n",
    "    \n",
    "    print \"|w_scipy - w_ols|^2 is\"\n",
    "    print np.linalg.norm(res.x - w_reg)\n",
    "\n",
    "    plot(lr,w)\n",
    "    \n",
    "    return (lr, gd, w, sae)\n",
    "\n",
    "\n",
    "def modelSelection(M, showPlot=False):\n",
    "    sseVal = lambda x: computeRidge(M, x, train_filename, test_filename, validate_filename, verbose=False)[0]\n",
    "    sseTest = lambda x: computeRidge(M, x, train_filename, test_filename, validate_filename, verbose=False)[1]\n",
    "    sseTrain = lambda x: computeRidge(M, x, train_filename, test_filename, validate_filename, verbose=False)[2]\n",
    "    \n",
    "    sseVal_vec = np.vectorize(sseVal)\n",
    "    sseTest_vec = np.vectorize(sseTest)\n",
    "    sseTrain_vec = np.vectorize(sseTrain)\n",
    "    \n",
    "    lam_vec = np.linspace(0,10,200)\n",
    "    a = sseVal_vec(lam_vec)\n",
    "    b = sseTest_vec(lam_vec)\n",
    "    c = sseTrain_vec(lam_vec)\n",
    "    \n",
    "    lam_min_idx = np.argmin(a)\n",
    "    lam_min = lam_vec[lam_min_idx]\n",
    "    \n",
    "    lr_train = lr_train = LinearRegression.fromFile(train_filename, M)\n",
    "    w_ridge = lr_train.ridge(lam_min)\n",
    "    \n",
    "\n",
    "    if showPlot:\n",
    "#         plt.plot(lam_vec, a, color='b')\n",
    "#         plt.plot(lam_vec, b, color='g')\n",
    "        plot(lr_train, w_ridge, plot_sin=False, plot_test=True, plot_validate=True)\n",
    "        plt.show()\n",
    "        \n",
    "    print \" \"\n",
    "    print \"-------------\"\n",
    "    print \"M = \" + str(M)\n",
    "    print \"argmin lambda = \" + str(lam_min)\n",
    "    print \"SSE_train = \" + str(c[lam_min_idx])\n",
    "    print \"SSE_val = \" + str(a[lam_min_idx])\n",
    "    print \"SSE_test = \" + str(b[lam_min_idx])\n",
    "    print \"----------------\"\n",
    "    print \" \"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filename = 'regress_train.txt'\n",
    "M = 1\n",
    "\n",
    "test_filename = \"regress_test.txt\"\n",
    "train_filename = \"regress_train.txt\"\n",
    "validate_filename = \"regress_validate.txt\"\n",
    "\n",
    "\n",
    "(lr, gd, w, sse) = gradDescentSAE(M)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot(lr, w, plot_sin=True, plot_test=False, plot_validate=False):\n",
    "    # plot sin(2*phi*x) in green\n",
    "    x_min = np.amin(lr.x)\n",
    "    x_max = np.amax(lr.x)\n",
    "    x = np.linspace(-3,3,1000)\n",
    "    M = lr.numFeatures - 1\n",
    "    sin_x = np.sin(2*np.pi*x)\n",
    "    lr_temp = LinearRegression(x,x,lr.numFeatures-1)\n",
    "    reg_prediction = np.dot(lr_temp.phi,w)\n",
    "    \n",
    "    lr_validate = LinearRegression.fromFile(validate_filename, M)\n",
    "    lr_test = LinearRegression.fromFile(test_filename, M)\n",
    "    \n",
    "    \n",
    "    if plot_sin:\n",
    "        plt.plot(x,sin_x, color='g')\n",
    "        \n",
    "    plt.plot(x, reg_prediction, color='r')\n",
    "    \n",
    "    # the training set is plotted in blue\n",
    "    plt.scatter(lr.x, lr.y, color='r', marker='o',facecolors='none')\n",
    "    \n",
    "    # test set plotted in green\n",
    "    if plot_test:\n",
    "        plt.scatter(lr_test.x, lr_test.y, color='g', marker='o',facecolors='none')\n",
    "        \n",
    "    # validation set plotted in orange\n",
    "    if plot_test:\n",
    "        plt.scatter(lr_validate.x, lr_validate.y, color='b', marker='o',facecolors='none')\n",
    "        \n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "def plotRidge(M,lam):\n",
    "    lr = LinearRegression.fromFile(filename, M)\n",
    "    w_ridge = lr.ridge(lam)\n",
    "    sse = lr.SSE(w_ridge)\n",
    "    plot(lr,w_ridge)\n",
    "    print \" \"\n",
    "    print \"--Ridge Regression Statistics--\"\n",
    "    print \"w_ridge = \" + str(w_ridge)\n",
    "    print \"SSE = \" + str(sse) \n",
    "    \n",
    "\n",
    "def computeRidge(M, lam, train, test, validate, verbose=True):\n",
    "    lr_train = LinearRegression.fromFile(train, M)\n",
    "    lr_validate = LinearRegression.fromFile(validate, M)\n",
    "    lr_test = LinearRegression.fromFile(test, M)\n",
    "    \n",
    "    w_ridge = lr_train.ridge(lam)\n",
    "    sse_train = lr_train.SSE(w_ridge)\n",
    "    sse_test = lr_test.SSE(w_ridge)\n",
    "    sse_validate = lr_validate.SSE(w_ridge)\n",
    "    \n",
    "    if verbose:\n",
    "        print \"--Ridge Regression Statistics--\"\n",
    "        print \"w_ridge = \" + str(w_ridge)\n",
    "        print \"SSE_train = \" + str(sse_train)\n",
    "        print \"SSE_validate = \" + str(sse_validate)\n",
    "        print \"SSE_test = \" + str(sse_test)\n",
    "\n",
    "        plot(lr_train, w_ridge, plot_sin=False, plot_test=True, plot_validate=True)\n",
    "    \n",
    "    return (sse_validate, sse_test, sse_train)\n",
    "\n",
    "def modelSelection(M, showPlot=False):\n",
    "    sseVal = lambda x: computeRidge(M, x, train_filename, test_filename, validate_filename, verbose=False)[0]\n",
    "    sseTest = lambda x: computeRidge(M, x, train_filename, test_filename, validate_filename, verbose=False)[1]\n",
    "    sseTrain = lambda x: computeRidge(M, x, train_filename, test_filename, validate_filename, verbose=False)[2]\n",
    "    \n",
    "    sseVal_vec = np.vectorize(sseVal)\n",
    "    sseTest_vec = np.vectorize(sseTest)\n",
    "    sseTrain_vec = np.vectorize(sseTrain)\n",
    "    \n",
    "    lam_vec = np.linspace(0,10,200)\n",
    "    a = sseVal_vec(lam_vec)\n",
    "    b = sseTest_vec(lam_vec)\n",
    "    c = sseTrain_vec(lam_vec)\n",
    "    \n",
    "    lam_min_idx = np.argmin(a)\n",
    "    lam_min = lam_vec[lam_min_idx]\n",
    "    \n",
    "    lr_train = lr_train = LinearRegression.fromFile(train_filename, M)\n",
    "    w_ridge = lr_train.ridge(lam_min)\n",
    "    \n",
    "\n",
    "    if showPlot:\n",
    "#         plt.plot(lam_vec, a, color='b')\n",
    "#         plt.plot(lam_vec, b, color='g')\n",
    "        plot(lr_train, w_ridge, plot_sin=False, plot_test=True, plot_validate=True)\n",
    "        plt.show()\n",
    "        \n",
    "    print \" \"\n",
    "    print \"-------------\"\n",
    "    print \"M = \" + str(M)\n",
    "    print \"argmin lambda = \" + str(lam_min)\n",
    "    print \"SSE_train = \" + str(c[lam_min_idx])\n",
    "    print \"SSE_val = \" + str(a[lam_min_idx])\n",
    "    print \"SSE_test = \" + str(b[lam_min_idx])\n",
    "    print \"----------------\"\n",
    "    print \" \"\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "filename = \"curvefitting.txt\"\n",
    "test_filename = \"regress_test.txt\"\n",
    "train_filename = \"regress_train.txt\"\n",
    "validate_filename = \"regress_validate.txt\"\n",
    "M = 1\n",
    "lam =1.14\n",
    "# plotRidge(M, lam)\n",
    "computeLAD(M, lam, train_filename, test_filename, validate_filename, verbose=True)\n",
    "\n",
    "# sseVal = lambda x: computeRidge(M, x, train_filename, test_filename, validate_filename, verbose=False)[0]\n",
    "# sseTest = lambda x: computeRidge(M, x, train_filename, test_filename, validate_filename, verbose=False)[1]\n",
    "\n",
    "# sseVal_vec = np.vectorize(sseVal)\n",
    "# sseTest_vec = np.vectorize(sseTest)\n",
    "\n",
    "# lam_vec = np.linspace(0,10,100)\n",
    "# a = sseVal_vec(lam_vec)\n",
    "# b = sseTest_vec(lam_vec)\n",
    "\n",
    "# plt.plot(lam_vec, a, color='b')\n",
    "# plt.plot(lam_vec, b, color='g')\n",
    "\n",
    "for m in range(0,6):\n",
    "    modelSelection(m, showPlot=False)\n",
    "    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sseVal_vec(np.array([1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blog Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_train = LinearRegression.fromBlog(type='train')\n",
    "lr_validate = LinearRegression.fromBlog(type='val')\n",
    "lr_test = LinearRegression.fromBlog(type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeBlogRidge(lam, verbose=True):\n",
    "    \n",
    "    print \"evaluating for a specific lambda\"\n",
    "    \n",
    "    w_ridge = lr_train.ridge(lam)\n",
    "    mse_train = lr_train.MSE(w_ridge)\n",
    "    mse_test = lr_test.MSE(w_ridge)\n",
    "    mse_validate = lr_validate.MSE(w_ridge)\n",
    "    \n",
    "    if verbose:\n",
    "        print \"--Ridge Regression Statistics--\"\n",
    "#         print \"w_ridge = \" + str(w_ridge)\n",
    "        print \"MSE_train = \" + str(mse_train)\n",
    "        print \"MSE_validate = \" + str(mse_validate)\n",
    "        print \"MSE_test = \" + str(mse_test)\n",
    "    \n",
    "    return (mse_validate, mse_test, mse_train)\n",
    "\n",
    "def blogModelSelection(showPlot=True):\n",
    "    sseVal = lambda x: computeBlogRidge(x, verbose=False)[0]\n",
    "    sseTest = lambda x: computeBlogRidge(x, verbose=False)[1]\n",
    "    sseTrain = lambda x: computeBlogRidge(x, verbose=False)[2]\n",
    "    num_obs = 1.0*np.shape(lr_train.phi)[0]\n",
    "    \n",
    "    sseVal_vec = np.vectorize(sseVal)\n",
    "    sseTest_vec = np.vectorize(sseTest)\n",
    "    sseTrain_vec = np.vectorize(sseTrain)\n",
    "    \n",
    "    lam_vec = num_obs*np.linspace(0.001,20,10)\n",
    "    a = sseVal_vec(lam_vec)\n",
    "    b = sseTest_vec(lam_vec)\n",
    "    c = sseTrain_vec(lam_vec)\n",
    "    \n",
    "    lam_min_idx = np.argmin(a)\n",
    "    lam_min = lam_vec[lam_min_idx]\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    if showPlot:\n",
    "#         plt.plot(lam_vec, c, color='r')\n",
    "#         plt.plot(lam_vec/num_obs, a, color='b')\n",
    "        plt.plot(lam_vec, b, color='g')\n",
    "        plt.show()\n",
    "        \n",
    "    print \" \"\n",
    "    print \"-------------\"\n",
    "    print \"argmin lambda = \" + str(lam_min)\n",
    "    print \"argmin lambda/num_obs = \" + str(lam_min/(num_obs))\n",
    "    print \"MSE_train = \" + str(c[lam_min_idx])\n",
    "    print \"MSE_val = \" + str(a[lam_min_idx])\n",
    "    print \"MSE_test = \" + str(b[lam_min_idx])\n",
    "    print \"----------------\"\n",
    "    print \" \" \n",
    "    \n",
    "\n",
    "# lr_train = LinearRegression.fromBlog(type='train')\n",
    "blogModelSelection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_obs = 31437\n",
    "computeBlogRidge(num_obs*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.67698062e+04  -3.34443445e+04   7.48398300e+03  -6.99489306e+02\n",
      "  -2.91446092e+02   6.92398706e+01   2.40443382e+01  -3.28916431e+01\n",
      "   2.01452964e+01  -2.24491834e+00  -3.07439085e-01  -8.89986340e+00]\n",
      "   status: 0\n",
      "  success: True\n",
      "     njev: 26\n",
      "     nfev: 364\n",
      " hess_inv: array([[   18.91138   ,    19.84670096,    26.2808625 ,     9.87173409,\n",
      "          -27.03159277,   -65.3924017 ,   -76.90199791,   -44.67070209,\n",
      "           16.08948149,    55.57462321,    14.54909104,  -140.55196022],\n",
      "       [   19.84670096,    22.99072587,    29.11862681,    10.94621826,\n",
      "          -29.90935887,   -72.36344587,   -85.08624927,   -49.39902801,\n",
      "           17.85204628,    61.54867501,    16.13856306,  -155.52866864],\n",
      "       [   26.2808625 ,    29.11862681,    39.5535919 ,    14.54640444,\n",
      "          -39.36778131,   -95.31815207,  -112.01142793,   -64.89470742,\n",
      "           23.77602584,    81.35846461,    21.47904668,  -204.84403347],\n",
      "       [    9.87173409,    10.94621826,    14.54640444,     6.84438827,\n",
      "          -13.6567195 ,   -33.72086951,   -39.50939255,   -22.37295575,\n",
      "            9.56707059,    30.25400408,     8.7505116 ,   -72.58237252],\n",
      "       [  -27.03159277,   -29.90935887,   -39.36778131,   -13.6567195 ,\n",
      "           44.97359595,   104.14014622,   122.54418991,    72.45105264,\n",
      "          -22.51853666,   -84.26410388,   -19.83530211,   223.24224457],\n",
      "       [  -65.3924017 ,   -72.36344587,   -95.31815207,   -33.72086951,\n",
      "          104.14014622,   248.64335895,   291.07695459,   171.04604246,\n",
      "          -55.77837364,  -203.09242952,   -49.28161993,   530.96983423],\n",
      "       [  -76.90199791,   -85.08624927,  -112.01142793,   -39.50939255,\n",
      "          122.54418991,   291.07695459,   342.93904747,   200.79491249,\n",
      "          -65.68547764,  -238.66394236,   -57.87181291,   623.90206606],\n",
      "       [  -44.67070209,   -49.39902801,   -64.89470742,   -22.37295575,\n",
      "           72.45105264,   171.04604246,   200.79491249,   119.18720998,\n",
      "          -37.75426461,  -138.91582588,   -32.9453471 ,   366.27434572],\n",
      "       [   16.08948149,    17.85204628,    23.77602584,     9.56707059,\n",
      "          -22.51853666,   -55.77837364,   -65.68547764,   -37.75426461,\n",
      "           15.79172037,    48.96365678,    13.64177344,  -120.26798894],\n",
      "       [   55.57462321,    61.54867501,    81.35846461,    30.25400408,\n",
      "          -84.26410388,  -203.09242952,  -238.66394236,  -138.91582588,\n",
      "           48.96365678,   171.93015543,    43.82221849,  -436.23694835],\n",
      "       [   14.54909104,    16.13856306,    21.47904668,     8.7505116 ,\n",
      "          -19.83530211,   -49.28161993,   -57.87181291,   -32.9453471 ,\n",
      "           13.64177344,    43.82221849,    13.38198774,  -106.4800046 ],\n",
      "       [ -140.55196022,  -155.52866864,  -204.84403347,   -72.58237252,\n",
      "          223.24224457,   530.96983423,   623.90206606,   366.27434572,\n",
      "         -120.26798894,  -436.23694835,  -106.4800046 ,  1138.79375431]])\n",
      "      fun: 1.0342609901029666e-08\n",
      "        x: array([  2.67684894e+04,  -3.34457045e+04,   7.48263875e+03,\n",
      "        -6.99534252e+02,  -2.90315467e+02,   7.02650863e+01,\n",
      "         2.39784104e+01,  -3.37637345e+01,   1.94908089e+01,\n",
      "        -2.20473022e+00,  -7.16288129e-02,  -8.98505520e+00])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "      jac: array([ -3.09946268e-06,  -3.49175681e-06,  -4.86081858e-06,\n",
      "        -3.24226888e-06,   6.26876468e-07,   4.95892034e-06,\n",
      "         7.96394061e-06,   8.46862137e-06,   6.17682297e-06,\n",
      "         1.71417680e-06,  -3.33473782e-06,  -6.73563334e-06])\n",
      "[  2.67684894e+04  -3.34457045e+04   7.48263875e+03  -6.99534252e+02\n",
      "  -2.90315467e+02   7.02650863e+01   2.39784104e+01  -3.37637345e+01\n",
      "   1.94908089e+01  -2.20473022e+00  -7.16288129e-02  -8.98505520e+00]\n"
     ]
    }
   ],
   "source": [
    "(lasso,_,_,_,_) = LinearRegression.fromLASSOData()\n",
    "np.shape(lasso.phi)\n",
    "\n",
    "\n",
    "w_reg = lasso.reg()\n",
    "print w_reg\n",
    "\n",
    "w = lasso.Lasso(0, w_0=w_reg)\n",
    "print w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
