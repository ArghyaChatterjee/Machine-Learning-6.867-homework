{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from neural_net import NeuralNet\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import scipy.io\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = \"toy_multiclass_1\"\n",
    "nn = NeuralNet.fromMAT(filename, type=\"train\")\n",
    "# print nn.X\n",
    "# print nn.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function eval = -80.8695162585\n",
      "(3, 31)\n",
      "function eval after gd step = -81.2521259333\n"
     ]
    }
   ],
   "source": [
    "gd = nn.constructGradDescentObject()\n",
    "w_list = [nn.W1, nn.W2]\n",
    "print \"function eval = \" + str(gd.evalF(w_list))\n",
    "gradient = gd.evalGradient(w_list)\n",
    "print np.shape(gradient[1])\n",
    "(w_new, f_new) = gd.gradDescentUpdate(w_list)\n",
    "print \"function eval after gd step = \" + str(gd.evalF(w_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape(T) = (3, 300)\n",
      "shape(X) = (3, 300)\n",
      "shape(W1) = (30, 3)\n",
      "shape(W2) = (3, 31)\n"
     ]
    }
   ],
   "source": [
    "print \"shape(T) = \" + str(np.shape(nn.T))\n",
    "print \"shape(X) = \" + str(np.shape(nn.X))\n",
    "print \"shape(W1) = \" + str(np.shape(nn.W1))\n",
    "print \"shape(W2) = \" + str(np.shape(nn.W2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M = 30\n",
      "shape(a_hidden) = (30, 300)\n",
      "shape(z) = (31, 300)\n",
      "shape(a_outputs) = (3, 300)\n",
      "shape(y) = (3, 300)\n"
     ]
    }
   ],
   "source": [
    "nn.forwardProp(nn.X)\n",
    "print \"M = \" + str(nn.M)\n",
    "print \"shape(a_hidden) = \" + str(np.shape(nn.a_hidden))\n",
    "print \"shape(z) = \" + str(np.shape(nn.z))\n",
    "print \"shape(a_outputs) = \" + str(np.shape(nn.a_outputs))\n",
    "print \"shape(y) = \" + str(np.shape(nn.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape(W1_grad) = (30, 3)\n",
      "shape(W2_grad) = (3, 31)\n",
      "shape(deltaHidden) = (30, 300)\n",
      "shape(deltaOutput) = (3,)\n"
     ]
    }
   ],
   "source": [
    "w_list = [nn.W1, nn.W2]\n",
    "[W1_grad, W2_grad] = nn.evalDerivs(w_list)\n",
    "print \"shape(W1_grad) = \" + str(np.shape(W1_grad))\n",
    "print \"shape(W2_grad) = \" + str(np.shape(W2_grad))\n",
    "print \"shape(deltaHidden) = \" + str(np.shape(nn.deltaHidden))\n",
    "print \"shape(deltaOutput) = \" + str(np.shape(nn.deltaOutput))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-61.739032517003473"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.evalCost(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2]\n",
    "b = []\n",
    "b.append(2)\n",
    "print b\n",
    "# type(a)\n",
    "# if type(a) == list:\n",
    "#     print \"we have a list!!!\"\n",
    "    \n",
    "# for idx, val in enumerate(a):\n",
    "#     print \"idx is \" + str(idx)\n",
    "#     print \"val is \" + str(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "We want to compute the loss function. Using the softmax formulation from Bishop the likelihood is $$\\prod_n y_n(x,w)^{t_n}$$. Then we get the log likelihood just by taking logs, then we put a negative sign in front in order to convert it to a minimization problem. Then the loss function is $$-\\sum_n t_n \\log(y_n(x,w)) + \\lambda ||w||_2^2 $$"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Activation function is sigmoid\n",
    "\n",
    "$$ g(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each $W^{(1)}$ and $W^{(2)}$ (they call them $w^{(1)}$ and $w^{(2)}$) is a matrix of weights:\n",
    "\n",
    "\n",
    "$$ W^{(1)} = \\begin{bmatrix} w_{1,0} & w_{1,1} & ... & ... & w_{1,D}\\\\\n",
    "w_{2,0} &w_{2,1} & ... & ... & w_{2,D} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "w_{M,0} &w_{M,1} & ... & ... & w_{M,D}  \\end{bmatrix}$$\n",
    "\n",
    "$$ W^{(2)} = \\begin{bmatrix} w_{1,0} & w_{1,1} & ... & ... & w_{1,M}\\\\\n",
    "w_{2,0} &w_{2,1} & ... & ... & w_{2,M} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "w_{K,0} &w_{K,1} & ... & ... & w_{K,M}  \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "And we assume $M=N/10$ to start, AKA the amount of hidden units we have in our 1 hidden layer is $M$, and we initialize it to $N/10$, where is $N$ is the number of training examples we have\n",
    "\n",
    "For our toy dataset, we just have $D$ = 2\n",
    "\n",
    "The loss function is the NLL and is, given training data and parameters $w$:\n",
    "\n",
    "$$ l(w) = \\sum_{i=1}^N \\sum_{k=1}^K \\big[ -y_k^{(i)} \\log (h_k (x^{(i)}. w)) = (1 - y_k^{(i)}) \\log(1-(h_k(x^{(i)},w)) \\big] $$\n",
    "\n",
    "But in order to avoid overfitting, we add regularization terms using the Frobenius norm $$||A||_F$ and use as our cost function:\n",
    "\n",
    "$$ J(w) = l(w) + \\lambda(||w^{(1)}||^2_F + ||w^{(2)}||^2_F $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling in the holes\n",
    "\n",
    "The homework description is annoyingly a description of what to do but without enough info to be helpful.\n",
    "\n",
    "What we also need is the activations:\n",
    "\n",
    "$$ a_j^{(1)} = \\sum_{i=1}^d w_{ji}^{(1)}x_i + w_{j0}^{(1)}$$\n",
    "\n",
    "And use $g$ to calculate the \"feature\" for each unit:\n",
    "\n",
    "$$ z_j = g(a_j) $$\n",
    "\n",
    "\n",
    "To nicely vectorize our computation of the activations $a$, the first step is to augment our input data with a \"1\" for each training sample, so that we allow for the $M$ bias weights to be included in the matrix:\n",
    "\n",
    "$$ x_{aug} = \\begin{bmatrix} 1_{N \\times 1} &| & x\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now nicely vectorize our computation for the activations:\n",
    "    \n",
    "$$ a^{(1)} = W^{(1)} x_{aug} $$\n",
    "\n",
    "Where we note the dimensionality of each: \n",
    "- $x_{aug}$ is a vector of dimension $D+1 \\times 1$, where $D$ is the dimensionality of the input data\n",
    "- $W^{(1)}$ is a matrix of dimension $M \\times D+1$, where $M$ is the number of hidden units\n",
    "- $a^{(1)}$ is a vector of dimension $M \\times 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that $x_{aug}$ is just for one of the training data samples, $n = 1,...,N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing backprop\n",
    "\n",
    "The notes from class are not super clear, but Bishop 5.3 is.\n",
    "\n",
    "Here's what we'll do:\n",
    "\n",
    "- Apply an input vector $x_n$ to the network and forward propagate through the network using 5.48 and 5.49 in order to find the activations of all hidden and output units\n",
    "- Evaluate the $\\delta_k$ for all the output units using 5.54\n",
    "- Backpropagate the $\\delta$'s using 5.56 to obtain $\\delta_j$ for each hidden unit in the network\n",
    "- Use 5.53 to evaluate the required derivatives\n",
    "\n",
    "\n",
    "5.48 (should be implemented as a matrix multiplication as described above): $$a_j = \\sum_i w_{ji}z_i$$\n",
    "\n",
    "5.49 (is already vectorized as long as $h()$ accepts numpy arrays as input: $$z_j = h(a_j)$$\n",
    "\n",
    "5.54 (can just be implemented as one subtraction of K-dimensional vectors): $$\\delta_k = y_k - t_k$$\n",
    "\n",
    "5.56 for going from output to hidden layer: $$\\delta_j = h'(a_j)\\sum_k w_{kj} \\delta_k$$\n",
    "\n",
    "Can be vectorized as:\n",
    "\n",
    "$$\\delta_{prev layer} = h'(a_{prev layer}) \\ \\  .* \\ \\ W_{no bias weights}^T \\delta_{outputs}  $$\n",
    "\n",
    "Where .* is element-wise multiply, $\\delta_{outputs}$ is a $K$-dimensional vector, $W_{no bias weights}^T$ is $M \\times K$\n",
    "\n",
    "5.53: $$ \\frac{\\partial E_n}{\\partial w_{ji}} = \\delta_j z_i $$\n",
    "\n",
    "Which can be vectorized as on outer product:\n",
    "\n",
    "$$ \\frac{\\partial E_n}{\\partial W} = z \\delta^T$$ \n",
    "\n",
    "\n",
    "\n",
    "While implementing as a batch method, we finally sum over all input data samples:\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial E}{\\partial w_{ji}} = \\sum_{n} \\frac{\\partial E_n}{\\partial w_{ji}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          1.01180584 -0.05384574]\n"
     ]
    }
   ],
   "source": [
    "print nn.X[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn.forwardProp(nn.X[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 11)\n",
      "(11, 10)\n"
     ]
    }
   ],
   "source": [
    "W2 = np.ones((10,11))\n",
    "print np.shape(W2)\n",
    "print np.shape(W2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7310585786300049"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / (1 + np.exp(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,)\n",
      "(3, 30)\n",
      "(31, 3)\n"
     ]
    }
   ],
   "source": [
    "nn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
