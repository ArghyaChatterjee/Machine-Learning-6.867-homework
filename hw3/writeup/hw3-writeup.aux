\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newlabel{toy1_train}{{1}{3}{Classification performance of a batch-gradient-descent-trained neural network on the training data itself for ``toy multiclass 1''. M = 30 hidden nodes were used, with $\lambda $ = 0, step size of 5e-5, and max 3,000 iterations. Shown classification error rate is 0.033 percent}{figure.1}{}}
\newlabel{toy1_val}{{2}{3}{Classification performance of a batch-gradient-descent-trained neural network on the validation data for ``toy multiclass 1''. M = 30 hidden nodes were used, with $\lambda $ = 0, step size of 5e-5, and max 3,000 iterations. Shown classification error rate is 0.033 percent}{figure.2}{}}
\newlabel{toy2_train}{{3}{3}{Classification performance of a batch-gradient-descent-trained neural network on the training data itself for ``toy multiclass 2''. M = 30 hidden nodes were used, with $\lambda $ = 0, step size of 5e-5, and max 3,000 iterations. Shown classification error rate is 0}{figure.3}{}}
\newlabel{toy2_val}{{4}{4}{Classification performance of a batch-gradient-descent-trained neural network on the validation data for ``toy multiclass 2'' . M = 30 hidden nodes were used, with $\lambda $ = 0, step size of 5e-5, and max 3,000 iterations. Shown classification error rate is 6.0 percent}{figure.4}{}}
\newlabel{batch_toydataset2}{{5}{4}{The classification error rate on the train, test and validation datasets for different numbers of hidden nodes. Performed with batch gradient descent, step size - 5e-5, max 3,000 iterations. Results shown are the average of ten trials}{figure.5}{}}
\newlabel{SGD_toy_data_2_CER}{{6}{4}{The classification error rate on the train, test and validation datasets for different numbers of hidden nodes}{figure.6}{}}
\newlabel{SGD_toy_data_2_training_time}{{7}{4}{The classification error rate on the train, test and validation datasets for different numbers of hidden nodes}{figure.7}{}}
\newlabel{MNIST_batch_varyM}{{8}{5}{The classification error rate on the train, test and validation datasets for different numbers of hidden nodes for the MNIST dataset. Step size used was 1e-4, max iterations 5,000, and $\lambda =0$}{figure.8}{}}
\newlabel{MNIST_reg_batch}{{9}{5}{The classification error rate on the train, test and validation datasets for different regularization parameters, $\lambda $. Step size used was 1e-4, max iterations 5,000, and $M=150$}{figure.9}{}}
