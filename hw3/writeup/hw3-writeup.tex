%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2015 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2015,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass[10pt]{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{float}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bm}
\usepackage{amssymb,amsmath}


% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2015} with
% \usepackage[nohyperref]{icml2015} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage[accepted]{icml2015}
\usepackage{amsmath}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{todonotes}

\usepackage{amsbsy}

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2015}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:

% Saving space by deleting running title (does this actually save space?)
%\icmltitlerunning{Submission and Formatting Instructions for ICML 2015}

\begin{document} 

\twocolumn[

% Saving space by deleting title
%\icmltitle{Submission and Formatting Instructions for \\ 
%           International Conference on Machine Learning (ICML 2015)}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2015
% package.


% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{6.867, machine learning}

\vskip 0.3in
]

% Saving space by deleting abstract
%\begin{abstract} 
%The purpose of this document is to provide both the basic paper template and
%submission guidelines.
%\end{abstract} 

\section*{Neural Networks}

We implemented a simple 2-layer regularized neural network, trained it using gradient descent, and used it on the provided toy datasets and MNIST handwritten digits datasets.

We choose to use the softmax formulation as described by Bishop for our loss function.  Please note that although this is a different loss function than we were asked to use in our assignment description, there was a follow-up discussion on Piazza (see note @504 if not familiar) which clarified that softmax is actually the correct formulation for 1-of-K classification.

The likelihood according to softmax is:
%
%
\begin{equation}
p(\mathbf{t} \ | \ \mathbf{X}, \mathbf{w}) = \prod_{k=1}^K \prod_{n=1}^N y_k(\mathbf{x}_n,\mathbf{w})^{t_{nk}}
\end{equation}
%
%
And taking the negative log likelihood we have our unregularized loss function:
%
%
\begin{equation}
l(\mathbf{w})= - \sum_{k=1}^K \sum_{n=1}^N  t_{nk} \ln y_k(\mathbf{x}_n,\mathbf{w}) 
\end{equation}
%
%
We note that $\mathbf{w}$ can be considered a vector that represents all of the weights of the neural network, but it is preferable to think of the weights as being organized into two matrices, which we denote $W^{(1)}$ and $W^{(2)}$ and explain later in the context of forward propagation.  The matrix representation, however, is useful for including regularization in our final cost function, which is formulated via the Frobenius norm:
%
%
\begin{equation}
J(w) = l(w) + \lambda(||W^{(1)}||^2_F + ||W^{(2)}||^2_F)
\end{equation}
%
%

\subsection*{Gradient Calculation}

Having defined our cost function, we are now able to describe how the gradients $\nabla_{W^{(1)}} J(\mathbf{w})$ and $\nabla_{W^{(1)}} J(\mathbf{w})$ are calculated analytically.  Before this derivation, however, we note each of these terms are matrices whose elements are simply the partial derivative of the cost function with respect to that element itself (a scalar).

In order to calculate the gradients, we will use error backpropagation, for which we will follow Bishop's nice explanation and follow these sequence of steps:

\begin{enumerate}
 \item Forward propagation
 \item Evaluate $\delta_k$ for the output units
 \item Backpropagation of the $\delta$'s for each hidden unit
 \item Evaluate derivatives with $\frac{\partial E_n}{\partial w_{ji}}$
\end{enumerate}

\textit{1. Forward propagation}

With the weights represented as matrices, we can vectorize the computation of the unit activations, for example for the first layer:
%
%
\begin{equation}
A^{(1)} = W^{(1)} X_{aug}
\end{equation}
%
%
Where $X_{aug}$ is a $(D+1) \times N$ augmented matrix for the input data, for the purpose of including the bias input unit in the vectorized computation.  If we consider the original input data to be $X$, of dimension $D \times N$, where $D$ is the dimensionality of each sample input and $N$ is the number of sample inputs, then we form $X_{aug}$ by augmenting $X$ with a $1 \times N$ vector of 1s ($X_{aug} = \begin{bmatrix} 1_{N \times 1} &| & X^T\end{bmatrix} ^T$).  $W^{(1)}$ is then a $M \times (D+1)$ matrix that contains all of the weights from every input to every hidden unit, except the bias unit of course, and $a^{(1)}$ is a $M \times N$ matrix where each column vector is individually the weights for all of the unit activations, given one sample input.  The "output" of each unit is computed simply by applying the sigmoid function $g()$ element-wise to the matrix $A^{(1)}$:
%
%
\begin{equation}
Z^{(1)}_{ij} = g(A^{(1)}_{ij})
\end{equation}
%
%
Where as requested in the assignment, we use the logistic sigmoid function as our sigmoid function:
%
%
\begin{equation}
g(z) = \frac{1}{1 + e^{-z}}
\end{equation}
%
%
To forward propagate through the second layer to the output units, we similarly use $A^{(2)} = W^{(2)} Z_{aug}^{(1)}$, where $Z^{(1)}$ has been augmented with a vector of 1s to include the bias unit, and so we have $W^{(2)}$ of dimension $K \times (M+1)$.  At this point we also note that in order to do our 1-of-K classification, we have to transform the output from the form they were given in...





\subsection*{Implementing 2-Layer Neural Network}

\subsection*{Stochastic Gradient Descent}

\subsection*{Testing the Neural Network Code}

\subsection*{Testing the Neural Network Code}

\subsection*{MNIST Data (Parts 5 and 6)}



\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
